{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQtiPPhrOA3I"
      },
      "source": [
        "# Разбор проекта по модулям 1+2+3+4\n",
        "\n",
        "В этом проекте необходимо было построить модель линейно регрессии, и предсказать какой-либо параметр. Датасет можно было взять учебный (например, load_boston), либо взять датасет, подготовленный GeekBrains. Ниже мы рассмотрим различные варианты решения этой задачи.\n",
        "\n",
        "\n",
        "\n",
        "## Повторение\n",
        "\n",
        "_Линейная регрессия_ — алгоритм, предназначенный для предсказания непрерывной величины. Например, это может быть цена на какой-то товар, какой-то физический показатель или иная характеристика, представляющая собой, например, вещественное число. _Задачи регрессии_ — это как раз тип задач, в которых предсказываются такого рода величины.\n",
        "\n",
        "Для решения задач машинного обучения, и в частности алгоритма линейной регрессии, можно использовать библиотеку `scikit-learn` (сокращённо `sklearn`). В этой библиотеке помимо готовых реализаций алгоритмов и моделей машинного обучения есть также средства для предобработки данных и многое другое.\n",
        "\n",
        "Также в библиотеке `sklearn` имеется набор датасетов. Для решения поставленной задачи мы возьмём датасет, содержащий цены на недвижимость в Бостоне и различные характеристики этой недвижимости. Для начала загрузим этот датасет:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01GLAtjROA3L"
      },
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9sunYzsOA3N"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_boston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plVFwgM9OA3O"
      },
      "source": [
        "Функция `load_boston` загружает датасет. Загрузим его в переменную `boston`. Этот датасет представлен в виде словаря. Посмотрим, какие у этого словаря есть ключи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGBkGfYaOA3Q"
      },
      "source": [
        "boston = load_boston()\n",
        "\n",
        "boston.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpA1j1BIOA3S"
      },
      "source": [
        "Данные о недвижимости хранятся в массиве по ключу `data`. Посмотрим поближе на эти данные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORL--_HlOA3U"
      },
      "source": [
        "data = boston[\"data\"]\n",
        "\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOUcqyTwOA3V"
      },
      "source": [
        "Мы видим, что это массив из 506 строк и 13 столбцов. Здесь каждая строка отвечает какому-то объекту (в нашем случае — объекту недвижимости), а столбцы — каким-то его характеристикам. Названия этих признаков хранятся в массиве по ключу `feature_names`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hby0w58OA3W"
      },
      "source": [
        "feature_names = boston[\"feature_names\"]\n",
        "\n",
        "feature_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkMbp-RZOA3X"
      },
      "source": [
        "Не совсем ясно, что представляют из себя эти признаки. Описание всего датасета можно получить по ключу `DESCR`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOHah0SGOA3Z"
      },
      "source": [
        "print(boston[\"DESCR\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU9fghYNOA3b"
      },
      "source": [
        "Данный текст содержит более подробную информацию о датасете, признаках, а также об авторах.\n",
        "\n",
        "Наконец, массив с целевыми значениями (в нашем случае — ценами на недвижимость) можно получить по ключу `target`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGR58cdMOA3d",
        "outputId": "efa616ff-26ab-4d6e-f511-4870b01775ea"
      },
      "source": [
        "target = boston[\"target\"]\n",
        "\n",
        "target[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdYYZeMNOA3d"
      },
      "source": [
        "Создадим несколько таблиц `DataFrame` для более удобного хранения данных. В таблице `X` будут храниться признаки. В качестве названий для столбцов возьмём массив `feature_names`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ3lv5GWOA3d"
      },
      "source": [
        "X = pd.DataFrame(data, columns=feature_names)\n",
        "\n",
        "X.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5_iZA_HOA3f"
      },
      "source": [
        "Выведем информацию об этой таблице:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVX2sDdYOA3g"
      },
      "source": [
        "X.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8iX2rUSOA3g"
      },
      "source": [
        "Как видно из этой информации, данная таблица не содержит пропущенных значений.\n",
        "\n",
        "Теперь создадим таблицу `y`, в которую запишем целевые значения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGMzG5NEOA3i"
      },
      "source": [
        "y = pd.DataFrame(target, columns=[\"price\"])\n",
        "\n",
        "y.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVZzApDvOA3i"
      },
      "source": [
        "В этой таблице также нет пропущенных значений.\n",
        "\n",
        "### Разбиение выборки на тренировочную и тестовую\n",
        "\n",
        "Для решения задачи (и в принципе в машинном обучении) используют две выборки: тренировочную и тестовую. Первая нужна, чтобы обучить модель. Вторая — для проверки качества обученной модели: мы можем сравнить предсказанную на этих данных цену с реальной, поскольку она у нас тоже имеется.\n",
        "\n",
        "Разбиение данных на тренировочную и тестовую выборку можно выполнить с помощью функции `train_test_split` из модуля `sklearn.model_selection`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH8CW4iwOA3j"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SucwMIcVOA3k"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWvM8H16OA3k"
      },
      "source": [
        "С помощью параметра `test_size` можно указать, какую часть данных мы хотим выделить под тест. Если подать сюда число из отрезка $[0, 1)$, то оно будет интерпретироваться как доля тестовых объектов. Если же подать число большее или равное 1, это будет число объектов в тестовой выборке.\n",
        "\n",
        "### Построение модели\n",
        "\n",
        "Итак, загрузим модель линейной регрессии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0r2zjWIOA3l"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH5_vVBcOA3l"
      },
      "source": [
        "lr = LinearRegression()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHb1r6n2OA3m"
      },
      "source": [
        "Модель линейной регрессии по объекту $x = (x_1, \\dots, x_n)$ предсказывает значение целевой переменной, используя линейную функцию:\n",
        "\n",
        "$$f(x) = w_0 + w_1 \\cdot x_1 + \\dots + w_n \\cdot x_n.$$ \n",
        "\n",
        "Задача такой модели — в процессе обучения подобрать эти коэффициенты $w_i$ так, чтобы значение этой функции было как можно более близко к реальному целевому значению $y$ объекта $x$.\n",
        "\n",
        "Итак, чтобы обучить модель, мы используем метод `.fit`, в который передаём нашу тренировочную выборку:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qv8I2tA-OA3m"
      },
      "source": [
        "lr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "205qb-FGOA3o"
      },
      "source": [
        "Теперь, когда модель обучена, мы можем получить предсказанные значения на объектах `X_test` с помощью метода `.predict`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7oKv8UIOA3o"
      },
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "y_pred.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpYWg2oqOA3p"
      },
      "source": [
        "Создадим таблицу `DataFrame` чтобы сопоставить реальные значения с предсказанными. Поскольку массив `y_pred` является двумерным, переведём его в одномерный, используя метод `.flatten`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxtECOAiOA3p"
      },
      "source": [
        "check_test = pd.DataFrame({\n",
        "    \"y_test\": y_test[\"price\"],\n",
        "    \"y_pred\": y_pred.flatten(),\n",
        "})\n",
        "\n",
        "check_test.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1fJYf_8OA3q"
      },
      "source": [
        "### Оцениваем качество модели\n",
        "\n",
        "Чтобы оценить то, насколько отличаются реальные значения от предсказанных, создадим отдельный столбец с их разностями:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N59XzI9OA3q"
      },
      "source": [
        "check_test[\"error\"] = check_test[\"y_pred\"] - check_test[\"y_test\"]\n",
        "\n",
        "check_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks-PH3oDOA3r"
      },
      "source": [
        "Одной из основных метрик для оценки качества моделей регрессии является _средняя квадратическая ошибка_ или _mean squared error_, сокращённо _mse_. Она вычисляется как среднее от квадратов ошибок на каждом из объектов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpAWUjbHOA3s"
      },
      "source": [
        "mse1 = (check_test[\"error\"] ** 2).mean()\n",
        "\n",
        "mse1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryRXkrPdOA3t"
      },
      "source": [
        "Эта метрика есть в готовом виде в библиотеке `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mchWeFaKOA3t"
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "mean_squared_error(check_test[\"y_pred\"], check_test[\"y_test\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YElCxb9EOA3t"
      },
      "source": [
        "Также применяется _средняя абсолютная ошибка_. Она представляет из себя среднее не от квадратов ошибок, а от их модулей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlFtcqvMOA3t"
      },
      "source": [
        "(np.abs(check_test[\"error\"])).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TfxumqmOA3v"
      },
      "source": [
        "Данная метрика также доступна в готовом виде:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoD-z49FOA3v"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "mean_absolute_error(check_test[\"y_pred\"], check_test[\"y_test\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar0pbMPNOA3w"
      },
      "source": [
        "Эта метрика отличается от mse тем, что не так сильно реагирует на выбросы в данных, например, на те ситуации, в которых у некоторых объектов недвижимости сильно завышена или сильно занижена цена.\n",
        "\n",
        "### Коэффициенты линейной регрессии\n",
        "\n",
        "Посмотрим, какие коэффициенты подобрала наша модель на этапе обучения. Свободный коэффициент $w_0$ хранится в атрибуте `.intercept_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMY2jMEHOA3w"
      },
      "source": [
        "lr.intercept_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9OpjTqWOA3x"
      },
      "source": [
        "Остальные коэффициенты $w_1$, $\\dots$, $w_n$ хранятся в атрибуте `.coef_`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw3hfNPKOA3x"
      },
      "source": [
        "lr.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy09JqVROA3x"
      },
      "source": [
        "Давайте визуализируем то, какой вклад вносит каждый из этих коэффициентов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKuF0m9TOA3x"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = 6, 4\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIdd0GL-OA3z"
      },
      "source": [
        "plt.barh(feature_names, lr.coef_.flatten())\n",
        "\n",
        "plt.xlabel(\"Вес признака\")\n",
        "plt.ylabel(\"Признак\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpPhhcRROA3z"
      },
      "source": [
        "### Стандартизация признаков\n",
        "\n",
        "Не стоит торопиться с выводами о том, какой признак имеет наибольший вклад в итоговую цену. Если мы посмотрим на таблицу `X_train.describe()`, содержащую статистические данные по таблице `X_train`, мы убедимся, что значения по каждому признаку _не отмасштабированы_, т. е. имеют разный масштаб разброса значений (см. строки `mean` и `std`):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nMFaJsTOA30"
      },
      "source": [
        "X_train.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bZRarESOA31"
      },
      "source": [
        "Для решения этой проблемы можно применить _стандартизацию_ признаков. Для этого нужно от значений каждого признака отнять среднее значение этого признака, а затем поделить на среднее квадратическое отклонение этого признака:\n",
        "\n",
        "$$x_{scaled} = \\dfrac{x - x_{mean}}{\\sigma_x}$$\n",
        "\n",
        "После стандартизации каждый признак имеет среднее значение 0 и среднее квадратическое отклонение 1.\n",
        "\n",
        "Изучим инструменты из библиотеки `sklearn`, с помощью которых можно провести стандартизацию данных:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XMWqnmgOA32"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MOIN_42OA32"
      },
      "source": [
        "«Обучим» объект `scaler` на наших данных, а затем сразу же получим стандартизированные значения. Это можно сделать с помощью одного общего метода `.fit_transform`. (На самом деле этот метод включает в себя два метода: `.fit` и `.transform`.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E1T2kC8OA32"
      },
      "source": [
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
        "\n",
        "X_test_scaled = scaler.fit_transform(X_test)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29TeuhdOA32"
      },
      "source": [
        "Итак, попробуем обучить нашу модель заново, но уже на стандартизированных данных, и выведем аналогичную диаграмму, но уже для новой модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFtIOXA9OA34"
      },
      "source": [
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "plt.barh(feature_names, lr.coef_.flatten())\n",
        "\n",
        "plt.xlabel(\"Вес признака\")\n",
        "plt.ylabel(\"Признак\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMw9UYDoOA34"
      },
      "source": [
        "Мы видим, что теперь вес признаков имеют совершенно иные значения друг относительно друга.\n",
        "\n",
        "Важно отметить, что стандартизация важна не только для отбора признаков. Она является важным этапом предобработки данных, без которого многие алгоритмы будут работать некорректно.\n",
        "\n",
        "Попробуем обучить модель, используя лишь признаки, вес которых достаточно отличается от 0, и посмотрим, как это отразится на качестве модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sES7P2IcOA35"
      },
      "source": [
        "important_features = [\n",
        "    feature\n",
        "    for feature, weight in zip(feature_names, lr.coef_.flatten())\n",
        "    if np.abs(weight) > 0.5\n",
        "]\n",
        "\n",
        "print(important_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MILlVX_iOA35"
      },
      "source": [
        "lr.fit(\n",
        "    X_train_scaled.loc[:, important_features],\n",
        "    y_train\n",
        ")\n",
        "\n",
        "y_pred = lr.predict(X_test_scaled.loc[:, important_features])\n",
        "\n",
        "mse2 = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"mse до: {}\".format(mse1))\n",
        "print(\"mse после: {}\".format(mse2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsO9esuJOA36"
      },
      "source": [
        "Стандартизация и отбор признаков иногда позволяют немного уменьшить ошибку, хотя и не всегда.\n",
        "\n",
        "### Модели линейной регрессии с регуляризацией (дополнительная информация)\n",
        "\n",
        "В машинном обучении важной проблемой является _переобучение_, и нужно уметь бороться с переобучением обучаемой модели. Переобучение — это когда модель очень хорошо подстраивается под имеющиеся у неё тренировочные данные, однако, на тестовых данных показывает очень плохой результат.\n",
        "\n",
        "Одним из симптомов переобучения линейных моделей являются очень большие по модулю веса. Бороться с этим можно с помощью _регуляризации_. В библиотеке `sklearn` доступны уже готовые реализации моделей линейной регрессии с L1- и L2-регуляризацией. Это, соответственно, модели `Lasso` и `Ridge` из модуля `sklearn.linear_model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "preagTcIOA36"
      },
      "source": [
        "from sklearn.linear_model import Lasso, Ridge"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H42H_gGnOA36"
      },
      "source": [
        "При этом, коэффициент регуляризации можно задавать с помощью параметра `alpha` при инициализации этих моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5j9zw5AOA36"
      },
      "source": [
        "model = Lasso(alpha=0.03)\n",
        "\n",
        "model.fit(X_train_scaled.loc[:, important_features], y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_scaled.loc[:, important_features])\n",
        "\n",
        "mean_squared_error(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daIBcIEBOA37"
      },
      "source": [
        "На графике ниже изображено изменение весов признаков при увеличении коэффициента регуляризации `alpha` от $10^{-3}$ до $10$ для модели `Lasso` (L1-регуляризация). Чем больше значение `alpha`, тем сильнее регуляризация и тем сильнее модель «штрафует» за большую абсолютную величину признаков. \n",
        "\n",
        "Такой метод часто используется для отбора признаков: у менее ценных признаков гораздо раньше обнуляются веса."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJxZce5UOA37"
      },
      "source": [
        "n = 50\n",
        "\n",
        "coeffs = np.zeros((n, len(important_features)))\n",
        "alpha_list = np.logspace(-3, 1, n)\n",
        "\n",
        "for i, val in enumerate(alpha_list):\n",
        "    lasso = Lasso(alpha=val)\n",
        "    lasso.fit(X_train_scaled.loc[:, important_features], y_train)\n",
        "    \n",
        "    coeffs[i, :] = lasso.coef_.flatten()\n",
        "\n",
        "for i in range(len(important_features)):\n",
        "    plt.plot(alpha_list, coeffs[:, i])\n",
        "\n",
        "plt.title('Убывание абсолютных значений весов признаков \\n при увеличении коэффициента регуляризации alpha (Lasso)')\n",
        "\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('Вес признака')\n",
        "\n",
        "plt.legend(important_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEFHkM8AOA38"
      },
      "source": [
        "При использовании модели `Ridge` (L2-регуляризация) наблюдается аналогичный эффект, однако, абсолютные значения весов убывают более плавно и примерно одновременно друг с другом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5RfeX0LOA39"
      },
      "source": [
        "n = 50\n",
        "\n",
        "coeffs = np.zeros((n, len(important_features)))\n",
        "alpha_list = np.logspace(-3, 3.5, n)\n",
        "\n",
        "for i, val in enumerate(alpha_list):\n",
        "    ridge = Ridge(alpha=val)\n",
        "    ridge.fit(X_train_scaled.loc[:, important_features], y_train)\n",
        "    \n",
        "    coeffs[i, :] = ridge.coef_.flatten()\n",
        "\n",
        "for i in range(len(important_features)):\n",
        "    plt.plot(alpha_list, coeffs[:, i])\n",
        "\n",
        "plt.title('Убывание абсолютных значений весов признаков \\n при увеличении коэффициента регуляризации alpha (Ridge)')\n",
        "\n",
        "plt.xlabel('alpha')\n",
        "plt.ylabel('Вес признака')\n",
        "\n",
        "plt.legend(important_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJoF25COA4N"
      },
      "source": [
        "В первом столбце полученного массива стоит вероятность каждого объекта принадлежать классу 0, а во втором - вероятность принадлежать классу 1. Можно заметить, что сумма значений каждой строки равна 1.\n",
        "\n",
        "## Support Vector Machine (дополнительная информация)\n",
        "\n",
        "_Support Vector Machine_ или _Метод опорных векторов_ является одним из самых известных методов машинного обучения. Он включает в себя несколько алгоритмов, с помощью которых можно решать задачи как классификации, так и регрессии. \n",
        "\n",
        "В задаче классификации метод SVM стремится построить между объектами разных классов «линию» (в пространствах больших размерностей это называется _гиперплоскость_) так, чтобы максимизировать расстояние от этой «линии» до объектов разных классов. Во многих случаях такой метод работает лучше, чем логистическая регрессия.\n",
        "\n",
        "Алгоритмы метода опорных векторов расположены в модуле `svm` библиотеки `sklearn`. Импортируем из этого модуля модель `SVC` (Support Vector Classifier):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wlyaL96OA4Q"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYAf-7vOA4R"
      },
      "source": [
        "Алгоритмы метода SVM чувствительны к ненормализованным и нестандартизованным данным. Поэтому прежде чем строить модель, нам следует масштабировать признаки. Ранее мы уже познакомились со _стандартизацией_: тогда мы вычитали из признаков их среднее значение, а затем делили на среднее квадратическое отклонение. Здесь мы попробуем применить _нормализацию_, т. е. расположить признаки так, чтобы минимальное значение каждого признака оказалось равным 0, а максимальное - 1. Это можно сделать с помощью инструмента `MinMaxScaler` из модуля `sklearn.preprocessing`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2XsXHnDOA4R"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
        "\n",
        "X_valid = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DG5DtQnOA4R"
      },
      "source": [
        "Заметим, что для нормализации данных `X_valid` мы использовали не метод `.fit_transform`, а метод `.transform`. Это мотивировано тем, что мы хотели бы, чтобы значения признаков из массивов `X_train` и `X_valid` были согласованы, т. е. чтобы те значения, которые совпадали до нормализации, остались равными и после неё. Поэтому обучать модель заново на значениях из `X_valid` мы не хотим.\n",
        "\n",
        "Итак, создадим модель и обучим её на наших тренировочных данных, а затем проверим её качество на валидационной выборке:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hmk4LOkOA4S"
      },
      "source": [
        "clf = SVC(gamma=\"auto\")\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_valid)\n",
        "y_pred_train = clf.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KizDHeg5OA4T"
      },
      "source": [
        "Оценим точность предсказания:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQbP8E9LOA4T"
      },
      "source": [
        "accuracy_score(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtl-CSgsOA4T"
      },
      "source": [
        "accuracy_score(y_train, y_pred_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Si24pRe7OA4T"
      },
      "source": [
        "Как мы видим, здесь мы получили более плохую точность, чем ранее, когда использовали модель логистической регрессии. Попробуем получше настроить модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_BKWozoOA4V"
      },
      "source": [
        "## KNN (дополнительная информация)\n",
        "\n",
        "KNN расшифровывается как _K Nearest Neighbours_ или _K ближайших соседей_. Это один из самых простых в понимании методов машинного обучения. Представим каждый объект из рассматриваемых данных в виде точки в пространстве признаков. Метод KNN основан на предположении, что объекты одного класса расположены в пространстве близко друг к другу. Раз так, то можно классифицировать новый объект исходя из того, какие объекты находятся в пространстве рядом с ним.\n",
        "\n",
        "### Масштабирование признаков с использованием RobustScaler\n",
        "\n",
        "В первую очередь, раз мы работаем с расстояниями между объектами, нам необходимо отмасштабировать признаки так, чтобы они вносили в это расстояние соизмеримый вклад. Делать это мы будем с помощью `RobustScaler`. При таком подходе масштабирования признаков минимизируется влияние выбросов в данных. Это достигается за счёт того, что при масштабировании используется медиана и интерквартильный размах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQMoytJsOA4V"
      },
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdSKc2HBOA4W"
      },
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "cols_for_scaling = ['Age', 'SibSp', 'Parch', 'Fare']\n",
        "\n",
        "X_train[cols_for_scaling] = scaler.fit_transform(X_train[cols_for_scaling])\n",
        "X_valid[cols_for_scaling] = scaler.transform(X_valid[cols_for_scaling])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_891-emvOA4W"
      },
      "source": [
        "X_train[cols_for_scaling].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHl4mfwUOA4X"
      },
      "source": [
        "Как мы видим, после масштабирования мода каждого из масштабируемых признаков стала равной 0.\n",
        "\n",
        "### Классификация с помощью KNN\n",
        "\n",
        "Итак, построим нашу модель. Идея метода заключается в том, что, классифицируя некоторый объект, мы смотрим на `K` его ближайших соседей и смотрим, представителей какого класса среди этих соседей больше всего. К такому классу мы и относим классифицируемый объект.\n",
        "\n",
        "Построим модель для различных `K`, чтобы подобрать наиболее удачное значение."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6s4A8RhoOA4X"
      },
      "source": [
        "k_values = np.arange(1, 11)\n",
        "\n",
        "accuracy_on_valid = []\n",
        "accuracy_on_train = []\n",
        "\n",
        "for i, value in enumerate(k_values):\n",
        "    clf = KNeighborsClassifier(n_neighbors=value)\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = clf.predict(X_valid)\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "        \n",
        "    acc_valid = accuracy_score(y_valid, y_pred)\n",
        "    acc_train = accuracy_score(y_train, y_pred_train)\n",
        "    \n",
        "    if i % 2 == 0:\n",
        "        print('K = {}'.format(value))\n",
        "        print('\\tacc_valid = {}'.format(acc_valid))\n",
        "        print('\\tacc_train = {}\\n'.format(acc_train))\n",
        "    \n",
        "    accuracy_on_valid.append(acc_valid)\n",
        "    accuracy_on_train.append(acc_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWn1sR7COA4Y"
      },
      "source": [
        "plt.plot(k_values, accuracy_on_valid, label=\"valid\")\n",
        "plt.plot(k_values, accuracy_on_train, label=\"train\")\n",
        "\n",
        "plt.xlabel('Значение параметра K')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCKwNGhVOA4Z"
      },
      "source": [
        "Мы видим, что при малых значениях `K` модель склонна к переобучению. Оптимальном выбором является число $K = 6$. Построим модель ещё раз, используя эти знания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRqQR0soOA4Z",
        "outputId": "6076bf7d-8516-457a-a160-28bc74c19a51"
      },
      "source": [
        "clf = KNeighborsClassifier(n_neighbors=6)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_valid)\n",
        "\n",
        "accuracy_score(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8251121076233184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98nSHlCtOA4a"
      },
      "source": [
        "Полученная точность также выше, чем при использовании логистической регрессии."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2YvZ8CjOA4q"
      },
      "source": [
        "## Деревья решений и случайный лес (Дополнительная информация)\n",
        "\n",
        "_Дерево решений_ — один из самых легко интерпретируемых методов машинного обучения. При использовании этого метода по тренировочным данным строится бинарное дерево, в каждом узле которого происходит разбиение данных по значению какого-то признака. На конце каждой ветки дерева (т. е. в каждом её _листе_) располагается метка класса, к которому следует отнести те объекты, которые дошли до соответствующего листа."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCQsJTzcOA4q"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRP8zPrpOA4r"
      },
      "source": [
        "Разберёмся с тем, какие параметры есть у модели `DecisionTreeClassifier`, и как их настраивать. Первый интересующий нас параметр — максимальная глубина дерева или `max_depth`. Деревья большой глубины склонны к переобучению: модель просто слишком детально подстраивается под тренировочные данные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0SUisiqOA4r"
      },
      "source": [
        "max_depth_values = np.arange(2, 20)\n",
        "\n",
        "accuracy_on_valid = []\n",
        "accuracy_on_train = []\n",
        "\n",
        "for i, value in enumerate(max_depth_values):\n",
        "    clf = DecisionTreeClassifier(max_depth=value)\n",
        "    \n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    y_pred = clf.predict(X_valid)\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "        \n",
        "    acc_valid = accuracy_score(y_valid, y_pred)\n",
        "    acc_train = accuracy_score(y_train, y_pred_train)\n",
        "    \n",
        "    if i % 4 == 2:\n",
        "        print('K = {}'.format(value))\n",
        "        print('\\tacc_valid = {}'.format(acc_valid))\n",
        "        print('\\tacc_train = {}\\n'.format(acc_train))\n",
        "    \n",
        "    accuracy_on_valid.append(acc_valid)\n",
        "    accuracy_on_train.append(acc_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ZjPb5yOA4r"
      },
      "source": [
        "plt.rcParams['figure.figsize'] = 6, 4\n",
        "\n",
        "plt.plot(max_depth_values, accuracy_on_valid, label=\"valid\")\n",
        "plt.plot(max_depth_values, accuracy_on_train, label=\"train\")\n",
        "\n",
        "plt.xlabel('Значение параметра max_depth')\n",
        "plt.ylabel('Accuracy')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjMXKMaCOA4s"
      },
      "source": [
        "Это мы и наблюдаем: при достаточно больших значениях параметра `max_depth` точность на тренировочных данных почти достигает 1. На валидационных же данных точность достигает своего пика приблизительно на значении `max_depth = 7`, а затем начинает падать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g0tmVgHOA4s"
      },
      "source": [
        "clf = DecisionTreeClassifier(max_depth=7)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_valid)\n",
        "\n",
        "accuracy_score(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQZHch1nOA4t"
      },
      "source": [
        "Даже в лучшем случае мы получаем точность, которая уступает точности других рассмотренных нами моделей. \n",
        "\n",
        "Рассмотрим теперь алгоритм, основанный на деревьях решений, но являющийся более сложным, а именно со _Случайным лесом_ или _Random forest_.\n",
        "\n",
        "### Random Forest (Дополнительная информация)\n",
        "\n",
        "В этом алгоритме используется _ансамбль_ деревьев решений. Для каждого обучаемого дерева решений создаётся подвыборка из исходной тренировочной выборки, кроме того, при обучении каждого дерева используется лишь часть признаков. \n",
        "\n",
        "На этапе предсказания мы учитываем ответы, полученные каждым из деревьев, и выбираем тот ответ, за который «проголосовало» наибольшее количество деревьев. Это позволяет, в частности, уменьшить влияние переобучения каждого из деревьев."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3g2001bOA4t"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er3ICfDSOA4t"
      },
      "source": [
        "Рассмотрим несколько параметров данной модели:\n",
        "\n",
        "* `n_estimators` - число деревьев в ансамбле\n",
        "* `max_features` - максимальное число признаков, которое может быть использовано при построении каждого дерева\n",
        "* `max_depth` - максимальная глубина дерева\n",
        "\n",
        "Для того, чтобы выбрать подходящий набор параметров, можно использовать _Grid search_ или поиск по сетке. Этот метод используется для того, чтобы перебрать все возможные комбинации параметров и выбрать ту комбинацию, которая максимизирует выбранную нами метрику. Можно воспользоваться готовой реализацией из библиотеки `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFPVv7FbOA4u"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vFp5cc3OA4u"
      },
      "source": [
        "Зададим сетку параметров, по которой мы хотели бы провести поиск:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONcrKfikOA4u"
      },
      "source": [
        "parameters = {\n",
        "    'n_estimators': [150, 200, 250],\n",
        "    'max_features': np.arange(5, 9),\n",
        "    'max_depth': np.arange(5, 10),\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(),\n",
        "    param_grid=parameters,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxWBIX8KOA4u"
      },
      "source": [
        "`GridSearchCV` — это классификатор, который строится на основе модели `estimator`, пробегая все комбинации значений из `param_grid`. Для каждой комбинации параметров по кросс-валидации на указанном количестве _фолдов_ считается метрика, указанная в `scoring`. Наконец, выбирается та комбинация параметров, при которой выбранная метрика оказалась максимальной, и дальше для предсказания используется именно этот набор параметров.\n",
        "\n",
        "### Кросс-валидация (Дополнительная информация)\n",
        "\n",
        "При оценке каждой построенной в процессе модели используется _кросс-валидация_. Это метод, при котором вся обучающая выборка разбивается на заданное число _фолдов_ (частей), и по очереди каждый из этих фолдов выкидывается из тренировочной выборки и используется для валидации. Другими словами, если `cv=5`, то мы строим 5 моделей, для каждой из них выкидывая один из фолдов из обучающей выборки. Затем значения выбранной метрики усредняются на этих 5 моделях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phaKpbGtOA4v"
      },
      "source": [
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH7yIv93OA4w"
      },
      "source": [
        "Для ознакомления с результатами Grid search можно использовать атрибут `.cv_results_`. Удобнее всего визуализировать эти результаты в виде `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPV9Ct7xOA4w"
      },
      "source": [
        "cv_results = pd.DataFrame(clf.cv_results_)\n",
        "\n",
        "cv_results.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFB2Dx_eOA4w"
      },
      "source": [
        "Посмотрим, как выбранные нами параметры влияют на точность модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKJFEbPmOA4w"
      },
      "source": [
        "param_columns = [\n",
        "    column\n",
        "    for column in cv_results.columns\n",
        "    if column.startswith('param_')\n",
        "]\n",
        "\n",
        "score_columns = ['mean_test_score', 'mean_train_score']\n",
        "\n",
        "cv_results = (cv_results[param_columns + score_columns]\n",
        "              .sort_values(by=score_columns, ascending=False))\n",
        "\n",
        "cv_results.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1kNfMPCOA4x"
      },
      "source": [
        "Мы видим, что наилучшей точности модель достигает, если взять 150 деревьев глубины не более 6, и на каждом из них выбирать не более 6 признаков. При этом модель всё ещё даёт ощутимо более хороший результат на обучающей выборке, но уже не так сильно переобучается.\n",
        "\n",
        "Наилучшие параметры можно также посмотреть, используя атрибут `.best_params_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYctq4ULOA4x",
        "outputId": "1a6493a8-ca28-4654-f3fb-ca7de496bda1"
      },
      "source": [
        "clf.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'max_depth': 6, 'max_features': 6, 'n_estimators': 150}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlfeYQJnOA4y"
      },
      "source": [
        "Полученная в результате модель аналогична такой модели:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKTff7jQOA4z"
      },
      "source": [
        "clf = RandomForestClassifier(max_depth=6, max_features=6, n_estimators=150)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_valid)\n",
        "\n",
        "accuracy_score(y_valid, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}